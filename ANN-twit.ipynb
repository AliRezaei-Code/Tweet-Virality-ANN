{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Data Preprocessing\n",
    "1. Load the CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('Tweet_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Clean 'Tweet text'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to clean the tweet text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)   # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)      # Remove mentions\n",
    "    text = re.sub(r'\\d+', '', text)       # Remove numbers\n",
    "    text = re.sub(r'#', '', text)         # Remove the hash # sign\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespace\n",
    "    return text\n",
    "\n",
    "# Clean the 'Tweet text'\n",
    "data['cleaned_text'] = data['Tweet text'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Convert 'Tweet text' to numerical format (encoding):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100)  # Limit number of features to 100 for simplicity\n",
    "\n",
    "# Fit and transform the cleaned text\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(data['cleaned_text'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Combine NLP features with numerical features: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert TF-IDF features to DataFrame\n",
    "tfidf_features_df = pd.DataFrame(tfidf_features.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "\n",
    "# Combine with the rest of the numerical data, excluding non-numeric and target features\n",
    "numerical_features = data.select_dtypes(include=['int64', 'float64'])\n",
    "combined_features = pd.concat([tfidf_features_df, numerical_features], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Scale the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Scale the features\n",
    "scaled_features = scaler.fit_transform(combined_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Define the target variable 'virality'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define virality based on the number of impressions\n",
    "data['is_viral'] = data['impressions'] > 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Build and Train the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Adjust the input layer to match the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "input_dim = scaled_features.shape[1]\n",
    "\n",
    "# Initializing the ANN\n",
    "ann_virality = tf.keras.models.Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "ann_virality.add(tf.keras.layers.Dense(units=16, activation='relu', input_dim=input_dim))\n",
    "\n",
    "# Adding the output layer\n",
    "ann_virality.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Compile and Train the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - 0s 1ms/step - loss: 0.8346 - accuracy: 0.3355\n",
      "Epoch 2/100\n",
      "20/20 [==============================] - 0s 988us/step - loss: 0.6667 - accuracy: 0.6499\n",
      "Epoch 3/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.5386 - accuracy: 0.8071\n",
      "Epoch 4/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.4371 - accuracy: 0.9011\n",
      "Epoch 5/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.3548 - accuracy: 0.9530\n",
      "Epoch 6/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.2891 - accuracy: 0.9724\n",
      "Epoch 7/100\n",
      "20/20 [==============================] - 0s 919us/step - loss: 0.2368 - accuracy: 0.9789\n",
      "Epoch 8/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1943 - accuracy: 0.9887\n",
      "Epoch 9/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1613 - accuracy: 0.9919\n",
      "Epoch 10/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1343 - accuracy: 0.9984\n",
      "Epoch 11/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.1130 - accuracy: 0.9984\n",
      "Epoch 12/100\n",
      "20/20 [==============================] - 0s 936us/step - loss: 0.0959 - accuracy: 0.9984\n",
      "Epoch 13/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0822 - accuracy: 0.9984\n",
      "Epoch 14/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0708 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "20/20 [==============================] - 0s 990us/step - loss: 0.0613 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "20/20 [==============================] - 0s 901us/step - loss: 0.0535 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "20/20 [==============================] - 0s 946us/step - loss: 0.0471 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "20/20 [==============================] - 0s 951us/step - loss: 0.0417 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "20/20 [==============================] - 0s 965us/step - loss: 0.0372 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0333 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "20/20 [==============================] - 0s 939us/step - loss: 0.0300 - accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "20/20 [==============================] - 0s 941us/step - loss: 0.0272 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "20/20 [==============================] - 0s 956us/step - loss: 0.0246 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "20/20 [==============================] - 0s 854us/step - loss: 0.0224 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "20/20 [==============================] - 0s 864us/step - loss: 0.0205 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "20/20 [==============================] - 0s 864us/step - loss: 0.0188 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0172 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "20/20 [==============================] - 0s 936us/step - loss: 0.0158 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "20/20 [==============================] - 0s 862us/step - loss: 0.0146 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0135 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "20/20 [==============================] - 0s 965us/step - loss: 0.0126 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "20/20 [==============================] - 0s 897us/step - loss: 0.0117 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 34/100\n",
      "20/20 [==============================] - 0s 941us/step - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 35/100\n",
      "20/20 [==============================] - 0s 961us/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 36/100\n",
      "20/20 [==============================] - 0s 985us/step - loss: 0.0090 - accuracy: 1.0000\n",
      "Epoch 37/100\n",
      "20/20 [==============================] - 0s 942us/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 38/100\n",
      "20/20 [==============================] - 0s 927us/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 39/100\n",
      "20/20 [==============================] - 0s 894us/step - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 40/100\n",
      "20/20 [==============================] - 0s 1ms/step - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 41/100\n",
      "20/20 [==============================] - 0s 957us/step - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 42/100\n",
      "20/20 [==============================] - 0s 888us/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 43/100\n",
      "20/20 [==============================] - 0s 960us/step - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 44/100\n",
      "20/20 [==============================] - 0s 807us/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 45/100\n",
      "20/20 [==============================] - 0s 862us/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 46/100\n",
      "20/20 [==============================] - 0s 840us/step - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 47/100\n",
      "20/20 [==============================] - 0s 841us/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 48/100\n",
      "20/20 [==============================] - 0s 863us/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 49/100\n",
      "20/20 [==============================] - 0s 829us/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 50/100\n",
      "20/20 [==============================] - 0s 894us/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 51/100\n",
      "20/20 [==============================] - 0s 833us/step - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 52/100\n",
      "20/20 [==============================] - 0s 926us/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 53/100\n",
      "20/20 [==============================] - 0s 814us/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 54/100\n",
      "20/20 [==============================] - 0s 876us/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 55/100\n",
      "20/20 [==============================] - 0s 815us/step - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 56/100\n",
      "20/20 [==============================] - 0s 917us/step - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 57/100\n",
      "20/20 [==============================] - 0s 849us/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 58/100\n",
      "20/20 [==============================] - 0s 860us/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 59/100\n",
      "20/20 [==============================] - 0s 884us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 60/100\n",
      "20/20 [==============================] - 0s 891us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 61/100\n",
      "20/20 [==============================] - 0s 936us/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 62/100\n",
      "20/20 [==============================] - 0s 880us/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 63/100\n",
      "20/20 [==============================] - 0s 929us/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 64/100\n",
      "20/20 [==============================] - 0s 870us/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 65/100\n",
      "20/20 [==============================] - 0s 918us/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 66/100\n",
      "20/20 [==============================] - 0s 926us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 67/100\n",
      "20/20 [==============================] - 0s 850us/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 68/100\n",
      "20/20 [==============================] - 0s 895us/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 69/100\n",
      "20/20 [==============================] - 0s 838us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 70/100\n",
      "20/20 [==============================] - 0s 917us/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 71/100\n",
      "20/20 [==============================] - 0s 837us/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 72/100\n",
      "20/20 [==============================] - 0s 874us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 73/100\n",
      "20/20 [==============================] - 0s 850us/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 74/100\n",
      "20/20 [==============================] - 0s 902us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 75/100\n",
      "20/20 [==============================] - 0s 916us/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 76/100\n",
      "20/20 [==============================] - 0s 870us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 77/100\n",
      "20/20 [==============================] - 0s 944us/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 78/100\n",
      "20/20 [==============================] - 0s 868us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 79/100\n",
      "20/20 [==============================] - 0s 908us/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 80/100\n",
      "20/20 [==============================] - 0s 847us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 81/100\n",
      "20/20 [==============================] - 0s 947us/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 82/100\n",
      "20/20 [==============================] - 0s 928us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 83/100\n",
      "20/20 [==============================] - 0s 853us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 84/100\n",
      "20/20 [==============================] - 0s 904us/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 85/100\n",
      "20/20 [==============================] - 0s 850us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 86/100\n",
      "20/20 [==============================] - 0s 890us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 87/100\n",
      "20/20 [==============================] - 0s 863us/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 88/100\n",
      "20/20 [==============================] - 0s 895us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 89/100\n",
      "20/20 [==============================] - 0s 867us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 90/100\n",
      "20/20 [==============================] - 0s 915us/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 91/100\n",
      "20/20 [==============================] - 0s 925us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 92/100\n",
      "20/20 [==============================] - 0s 865us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 93/100\n",
      "20/20 [==============================] - 0s 890us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 94/100\n",
      "20/20 [==============================] - 0s 835us/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 95/100\n",
      "20/20 [==============================] - 0s 872us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 96/100\n",
      "20/20 [==============================] - 0s 821us/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 97/100\n",
      "20/20 [==============================] - 0s 910us/step - loss: 9.8431e-04 - accuracy: 1.0000\n",
      "Epoch 98/100\n",
      "20/20 [==============================] - 0s 855us/step - loss: 9.6172e-04 - accuracy: 1.0000\n",
      "Epoch 99/100\n",
      "20/20 [==============================] - 0s 896us/step - loss: 9.3863e-04 - accuracy: 1.0000\n",
      "Epoch 100/100\n",
      "20/20 [==============================] - 0s 888us/step - loss: 9.1793e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f082c92c5e0>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compiling the ANN\n",
    "ann_virality.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Splitting the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    scaled_features, data['is_viral'], test_size=0.2, random_state=0\n",
    ")\n",
    "\n",
    "# Training the ANN on the Training set\n",
    "ann_virality.fit(X_train, y_train, batch_size=32, epochs=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Evaluate the Model\n",
    "Evaluate the model performance with the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 543us/step\n",
      "[[155]]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = ann_virality.predict(X_test) > 0.5\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(cm)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to use the pretrained model on new tweets?\n",
    "\n",
    "To predict the probability of a new tweet going viral using the trained model, we will need to preprocess the new tweet data in the same way as the training data was preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali0rez/.local/lib/python3.10/site-packages/sklearn/base.py:464: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 100 features, but StandardScaler is expecting 119 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_45409/2198089123.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Example usage:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mnew_tweet_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Your new tweet text here.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mpreprocessed_new_tweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_tweet_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_tweet_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mvirality_probability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_virality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mann_virality\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessed_new_tweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Probability of the tweet being viral: {virality_probability[0][0]:.2%}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_45409/2198089123.py\u001b[0m in \u001b[0;36mpreprocess_tweet_text\u001b[0;34m(new_tweet_text, tfidf_vectorizer, scaler)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Scale the TF-IDF features using the previously fitted scaler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mscaled_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mscaled_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1003\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m         X = self._validate_data(\n\u001b[0m\u001b[1;32m   1005\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m             \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ensure_2d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    415\u001b[0m                 \u001b[0;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m                 \u001b[0;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: X has 100 features, but StandardScaler is expecting 119 features as input."
     ]
    }
   ],
   "source": [
    "def preprocess_tweet_text(new_tweet_text, tfidf_vectorizer, scaler):\n",
    "    # Clean the new tweet text\n",
    "    cleaned_text = clean_text(new_tweet_text)\n",
    "    \n",
    "    # Transform the text using the previously fitted TF-IDF vectorizer\n",
    "    tfidf_features = tfidf_vectorizer.transform([cleaned_text])\n",
    "    \n",
    "    # Scale the TF-IDF features using the previously fitted scaler\n",
    "    scaled_features = scaler.transform(tfidf_features.toarray())\n",
    "    \n",
    "    return scaled_features\n",
    "\n",
    "# Predict the probability of a tweet being viral based on the text\n",
    "def predict_virality(model, preprocessed_text):\n",
    "    virality_probability = model.predict(preprocessed_text)\n",
    "    return virality_probability\n",
    "\n",
    "# Example usage:\n",
    "new_tweet_text = \"Your new tweet text here.\"\n",
    "preprocessed_new_tweet = preprocess_tweet_text(new_tweet_text, tfidf_vectorizer, scaler)\n",
    "virality_probability = predict_virality(ann_virality, preprocessed_new_tweet)\n",
    "print(f\"Probability of the tweet being viral: {virality_probability[0][0]:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the ratio of viral to non viral tweets in order to prevent overfitting: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of viral tweets in the training set: 3\n",
      "Balance ratio of viral to non-viral tweets in training set: 0.49%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Output the number of viral tweets in the training set\n",
    "viral_tweets_count = np.sum(y_train)\n",
    "print(f\"Number of viral tweets in the training set: {viral_tweets_count}\")\n",
    "\n",
    "# Check balance\n",
    "balance_ratio = viral_tweets_count / len(y_train)\n",
    "print(f\"Balance ratio of viral to non-viral tweets in training set: {balance_ratio:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New Code: \n",
    "\n",
    "\n",
    "\n",
    "Preparing the Dataset for Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# Assuming 'data' contains your dataset and 'clean_text' is your text cleaning function\n",
    "\n",
    "# Initialize and fit TF-IDF Vectorizer on cleaned tweet texts\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100)  # Adjust 'max_features' as needed\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(data['cleaned_text'].apply(clean_text))\n",
    "\n",
    "# Scale TF-IDF features\n",
    "scaler = StandardScaler().fit(tfidf_features.toarray())\n",
    "scaled_tfidf_features = scaler.transform(tfidf_features.toarray())\n",
    "\n",
    "# Define the target variable\n",
    "data['is_viral'] = data['impressions'] > 100000  # Example criterion for virality\n",
    "\n",
    "# Split the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    scaled_tfidf_features, \n",
    "    data['is_viral'], \n",
    "    test_size=0.2, \n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# Compute class weights for imbalance handling\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = dict(enumerate(class_weights))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adjusting the Neural Network Architecture\n",
    "\n",
    "Note : Ensure the input layer of your ANN matches the number of features in your TF-IDF vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Value passed to parameter 'x' has DataType bool not in list of allowed values: bfloat16, float16, float32, float64, int8, int16, int32, int64, complex64, complex128",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_45409/1206254806.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Training the ANN with class weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m ann_virality.fit(\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0mraises\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto\u001b[0m \u001b[0mkeep\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[0mshort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfocused\u001b[0m \u001b[0mon\u001b[0m \u001b[0mwhat\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0mactionable\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0myou\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0myour\u001b[0m \u001b[0mown\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0malso\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_traceback_filtering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;31m`\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable_traceback_filtering\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_SatisfiesTypeConstraint\u001b[0;34m(dtype, attr_def, param_name)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mallowed_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m       raise TypeError(\n\u001b[0m\u001b[1;32m     58\u001b[0m           \u001b[0;34mf\"Value passed to parameter '{param_name}' has DataType \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m           \u001b[0;34mf\"{dtypes.as_dtype(dtype).name} not in list of allowed values: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Value passed to parameter 'x' has DataType bool not in list of allowed values: bfloat16, float16, float32, float64, int8, int16, int32, int64, complex64, complex128"
     ]
    }
   ],
   "source": [
    "# Initializing the ANN\n",
    "ann_virality = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(units=16, activation='relu', input_dim=X_train.shape[1]),\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compiling the ANN\n",
    "ann_virality.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Training the ANN with class weights\n",
    "ann_virality.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    batch_size=32, \n",
    "    epochs=100, \n",
    "    class_weight=class_weight_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting Test Accuracy and Loss Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ali0rez/.local/lib/python3.10/site-packages/matplotlib/projections/__init__.py:63: UserWarning: Unable to import Axes3D. This may be due to multiple versions of Matplotlib being installed (e.g. as a system package and as a pip package). As a result, the 3D projection is not available.\n",
      "  warnings.warn(\"Unable to import Axes3D. This may be due to multiple versions of \"\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_45409/2434947352.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Plotting training & validation accuracy values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plotting training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inference with the Trained Model\n",
    "\n",
    "Note: Process the tweet text through the same cleaning, TF-IDF vectorization, and scaling steps before predicting with the trained model. This ensures the model predicts based on the tweet text alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_predict(new_tweet_text, tfidf_vectorizer, scaler, model):\n",
    "    # Clean and preprocess the new tweet text\n",
    "    cleaned_text = clean_text(new_tweet_text)\n",
    "    tfidf_features = tfidf_vectorizer.transform([cleaned_text])\n",
    "    scaled_features = scaler.transform(tfidf_features.toarray())\n",
    "    \n",
    "    # Predict virality\n",
    "    virality_probability = model.predict(scaled_features)\n",
    "    return virality_probability\n",
    "\n",
    "# Example usage\n",
    "new_tweet_text = \"Example tweet text here.\"\n",
    "probability = preprocess_and_predict(new_tweet_text, tfidf_vectorizer, scaler, ann_virality)\n",
    "print(f\"Probability of being viral: {probability[0][0]:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
